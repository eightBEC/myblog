[{"content":" # Intro Training LLMs is oftern limited by the available VRAM of your GPU or other resources like time. Unsloth is a great library that helps you train LLMs faster and with less memory. Based on their benchmarks up to 2x faster and with up to 80% less memory.\nThe following examples shows a minimum training code example and a Dockerfile you can use in your environment to get started training your models faster.\n# Prerequisites Docker / Podman # Example: Minimum Trainer Code for Unsloth The following example shows how to fine-tune your model with Unsloth. The code is put together based on the examples provided on Unsloth Github.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 # file: unsloth_trainer.py import os import torch from transformers import TrainingArguments from trl import SFTTrainer, DataCollatorForCompletionOnlyLM from datasets import Dataset, load_from_disk from unsloth import FastLanguageModel MODEL_ID = \u0026#34;unsloth/gemma-7b-bnb-4bit\u0026#34; # Quantized models from unsloth for faster downloading TRAINING_DATA_PATH = \u0026#34;path/to/training-dataset\u0026#34; OUTPUT_DATA_PATH = \u0026#34;path/where/model/is/stored\u0026#34; NUM_EPOCHS = 1 # Load model model, tokenizer = FastLanguageModel.from_pretrained( model_name=MODEL_ID, max_seq_length=2048, # adjust to your sequence length dtype=None, load_in_4bit=True, ) # Do model patching and add fast LoRA weights model = FastLanguageModel.get_peft_model( model, r=16, target_modules=[ \u0026#34;q_proj\u0026#34;, \u0026#34;k_proj\u0026#34;, \u0026#34;v_proj\u0026#34;, \u0026#34;o_proj\u0026#34;, \u0026#34;gate_proj\u0026#34;, \u0026#34;up_proj\u0026#34;, \u0026#34;down_proj\u0026#34;, ], lora_alpha=32, lora_dropout=0, bias=\u0026#34;none\u0026#34;, use_gradient_checkpointing=True, random_state=1133, use_rslora=False, loftq_config=None, ) dataset = ds.load_from_disk(TRAINING_DATA_PATH) sft_trainer = SFTTrainer( model=model, tokenizer=tokenizer, train_dataset=dataset, data_collator=data_collator, formatting_func=format_prompts_func, max_seq_length=max_seq_length, dataset_num_proc=2, packing=False, args=TrainingArguments( gradient_accumulation_steps=4, auto_find_batch_size=True, warmup_steps=5, num_train_epochs=NUM_EPOCHS, learning_rate=2.5e-5, fp16=not torch.cuda.is_bf16_supported(), bf16=torch.cuda.is_bf16_supported(), logging_steps=1, optim=\u0026#34;adamw_8bit\u0026#34;, weight_decay=0.01, lr_scheduler_type=\u0026#34;linear\u0026#34;, seed=1133, output_dir=OUTPUT_DATA_PATH, ), ) sft_trainer.train() try: model.save_pretrained_merged( os.path.join(OUTPUT_DATA_PATH, \u0026#34;model-16bit\u0026#34;), tokenizer, save_method=\u0026#34;merged_16bit\u0026#34;, ) except Exception as e: print(\u0026#34;Error saving merged_16bit model\u0026#34;) print(e) try: # Merge to 4bit model.save_pretrained_merged( os.path.join(OUTPUT_DATA_PATH, \u0026#34;model-4bit\u0026#34;), tokenizer, save_method=\u0026#34;merged_4bit\u0026#34;, ) except Exception as e: print(\u0026#34;Error saving merged_4bit model\u0026#34;) print(e) try: # Just LoRA adapters model.save_pretrained_merged( os.path.join(OUTPUT_DATA_PATH, \u0026#34;model-lora\u0026#34;), tokenizer, save_method=\u0026#34;lora\u0026#34;, ) except Exception as e: print(\u0026#34;Error saving lora model\u0026#34;) print(e) # Example: Dockerfile for Unsloth This Dockerfile uses the NVIDIA CUDA base image to provide a stable foundation. If you wanto to run this on Red Hat OpenShift please remember to add the non-priveleged user accordingly.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 # Start from the NVIDIA CUDA base image FROM nvidia/cuda:12.1.0-base-ubuntu22.04 # Set a fixed model cache directory. ENV TORCH_HOME=/root/.cache/torch # Install Python and necessary packages RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ wget build-essential python3.10 python3-pip python3.10-dev \\ git \\ \u0026amp;\u0026amp; apt-get clean \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* # Update pip and setuptools RUN python3.10 -m pip install --upgrade pip setuptools wheel # Install miniconda ENV CONDA_DIR /opt/conda RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh \u0026amp;\u0026amp; \\ /bin/bash ~/miniconda.sh -b -p /opt/conda ENV PATH=$CONDA_DIR/bin:$PATH # Install PyTorch with CUDA 12.1 support and other essential packages # Use a dedicated conda env RUN conda create --name unsloth_env python=3.10 RUN echo \u0026#34;source activate unsloth_env\u0026#34; \u0026gt; ~/.bashrc ENV PATH /opt/conda/envs/unsloth_env/bin:$PATH # As described in the Unsloth.ai Github RUN conda install -n unsloth_env -y pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers RUN pip install \u0026#34;unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\u0026#34; RUN pip install matplotlib RUN pip install --no-deps trl peft accelerate bitsandbytes RUN pip install autoawq # Copy the fine-tuning script into the container COPY ./unsloth_trainer.py /trainer/unsloth.trainer.py WORKDIR /trainer # endless running task to avoid container to be stopped CMD [ \u0026#34;/bin/sh\u0026#34; , \u0026#34;-c\u0026#34;, \u0026#34;tail -f /dev/null\u0026#34; ] # Further information The sample python and Dockerfiles can also be found on my Github. For those of you interested in diving deeper, please refer to the Unsloth Github for the latest updates, models, etc. - This library is developing balzingly fast.\n","date":"2024-05-03T00:00:00Z","permalink":"https://iamjanforster.de/p/unsloth-fast-llm-training/","title":"Training LLMs faster and with less memory with Unsloth"},{"content":" # Intro In case you want to host your own LLM instance of popular models like Mistral, Llama-2 or your own fine-tuned version of one of these models, Hugginface Text Generation Inference (TGI) is a great tool to get the job done. Often this requires running your inference on a Kubernetes or Openshift cluster providing the necessary GPU infrastructure.\nIn this post, I\u0026rsquo;ll show a quick example of how to get your own LLM instance up and running on your Kubernetes cluster.\n# Prerequisite A suitable GPU (A10, A100, H100, L40s) available within your cluster, depending on the model you want to run. Usually this involves the installation of the NVIDIA Node Feature Discovery (NFD) Operator and the NVIDIA GPU operator to make the GPU available to your pod.\n# Example: Deploying Mistral using TGI in Kubernetes Running TGI on Kubernetes is pretty straightforward, once you\u0026rsquo;ve figured out the basic setup. To avoid downloading the model weights every time you restart the pod, create a PersistentVolumeClaim (PVC) to persist the model weights:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: hf-cache spec: accessModes: - ReadWriteMany resources: requests: storage: 50Gi selector: matchLabels: pv: local storageClassName: \u0026#34;\u0026#34; # your storage class, if omitted, the default storage class will be used Now that you have created a PVC, you can create the deployment using the following deployment yaml. This deployment will download your model weights, then start the text generation inference servcer. In this example, it will run the mistralai/Mistral-7B-Instruct-v0.2 model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 apiVersion: apps/v1 kind: Deployment metadata: name: tgi-mistral spec: replicas: 1 selector: matchLabels: app: tgi-mistral template: metadata: labels: app: tgi-mistral spec: volumes: - name: hub persistentVolumeClaim: claimName: hf-cache - name: dshm emptyDir: medium: Memory containers: - name: model image: ghcr.io/huggingface/text-generation-inference:1.4.0 command: [\u0026#34;bash\u0026#34;, \u0026#34;-c\u0026#34;] args: - text-generation-server download-weights mistralai/Mistral-7B-Instruct-v0.2; text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2 --port 8080 env: - name: PORT value: \u0026#34;8080\u0026#34; - name: HF_HUB_CACHE value: /data - name: HF_HOME value: /data volumeMounts: - mountPath: /dev/shm name: dshm - mountPath: /data name: hub resources: requests: cpu: 1.0 memory: 8Gi nvidia.com/gpu: 1 limits: cpu: 5.0 memory: 32Gi nvidia.com/gpu: 1 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: tgi-mistral-service labels: app: tgi-mistral-service spec: selector: app: tgi-mistral type: NodePort ports: - protocol: TCP port: 8080 Within your cluster, you can now reach the TGI service using the following curl command in another pod. Make sure to replace the deployment-namespace with the name of the namespace you deployed TGI to:\n1 2 3 4 curl http://tgi-mistral.\u0026lt;deployment-namespace\u0026gt;.svc.cluster.local::8080/generate \\ -X POST \\ -d \u0026#39;{\u0026#34;inputs\u0026#34;:\u0026#34;What is Deep Learning?\u0026#34;,\u0026#34;parameters\u0026#34;:{\u0026#34;max_new_tokens\u0026#34;:20}}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; If you want to expose TGI to the public internet, you can create a Route (Openshift) or Ingress (Kubernetes).\nPlease make sure to adjust the timeout of your Route/Ingress if you have long running inference jobs. Larger models like Mixtral might run for more than 60s - which is the default timeout for Routes/Ingress - for large prompts wiht long contexts.\n# Further information For those interested in diving deeper, please refer to the Hugginface TGI documentation for CLI parameters, Python SDK, etc.\n","date":"2024-03-15T00:00:00Z","permalink":"https://iamjanforster.de/p/huggingface-tgi-serving/","title":"Serving LLMs using Huggingface Text Generation Inference Server on Kubernetes"},{"content":" # Summary I\u0026rsquo;ve created a GitHub repository named \u0026ldquo;fastapi-ml-skeleton\u0026rdquo; to simplify the deployment of machine learning models into production. Utilizing FastAPI, this project offers a robust framework for serving models securely and efficiently. The repository includes a tested example using a regression model for demonstrating house price predictions, targeting Python 3.11+ environments and employing Poetry for dependency management.\nFastAPI creates a OpenAPI documention which you can directly use in Postman or generate code from for your client consuming this boilerplate code. # Highlights FastAPI Framework: Ensures fast, secure, and easy deployment. Tested Sample Code: Includes a regression model to get you started quickly. Comprehensive Documentation: Guides on installation, setup, and API usage. Open Source: Available under the Apache-2.0 license. # Usage To use this skeleton, clone the repository and follow the setup instructions provided. It covers everything from installation and running the local server to API authentication. This framework is designed to be scalable, allowing for easy expansion and integration of various machine learning models.\nFor more details, visit the GitHub repository.\nFor feedback, issues and pull requests, please refer to the linked repository.\n","date":"2023-11-10T00:00:00Z","permalink":"https://iamjanforster.de/p/fastapi-ml-skeleton/","title":"A simple FastAPI boilerplate for ML models"},{"content":" # Intro Working with IBM Watson Knowledge Studio and IBM Natural Language Understanding (NLU) can be complicated, especially when relying on complex entity and relationship structures. Therefore, I created wkstools as a simple wrappe to work with the data provided by NLU.\nwkstools is essentially a convenience library that bridges the gap between raw NLU outputs and actionable data insights. Its primary function is to provide a set of utilities that make it easier to parse, interpret, and utilize entities and relations generated by IBM Watson NLU, aiming to reduce the overhead involved in manually sifting through JSON data.\n# Usage Getting started with wkstools is straightforward. The library requires Python 3.6+ and Pydantic, ensuring a wide compatibility range. Installation is a breeze via pip, and the documentation provides clear examples of how to parse entities and relations from NLU JSON responses. This practical approach allows developers to quickly integrate wkstools into their projects, focusing more on leveraging NLU insights and less on data preprocessing.\n# Installation 1 $ pip install wkstools # Example: Parse entities and relations To parse the NLU JSON response retrieved from IBM Natural Language Understanding, use:\n1 2 3 4 5 6 7 import wkstools # Your NLU JSON response to process nlu_response = \u0026#39;{..., \u0026#34;relations\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;specifiesValue\u0026#34;, ...], \u0026#34;entities\u0026#34;: [...]}\u0026#39; entities = wkstools.parse_entities(nlu_response) relations = wkstools.parse_relations(nlu_response) See the entity and relation models for available fields.\n# Example: Access specfic relations 1 2 3 4 5 6 7 8 import wkstools # Your NLU JSON response to process nlu_response = \u0026#39;{..., \u0026#34;relations\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;specifiesValue\u0026#34;, ...], \u0026#34;entities\u0026#34;: [...]}\u0026#39; relations = wkstools.parse_relations(nlu_response) value_relations = wkstools.get_relations_by_type(relations, \u0026#34;specifiesValue\u0026#34;) # Further information For those interested in diving deeper or integrating wkstools into their own projects, my GitHub repository offers comprehensive guidance, from installation instructions to detailed usage examples. You can explore the repository, contribute, or simply learn more about how wkstools can enhance your NLU projects by visiting wkstools on GitHub.\n","date":"2020-08-09T00:00:00Z","permalink":"https://iamjanforster.de/p/wkstools/","title":"wkstools - A little helper for IBM Watson Natural Language Understanding with Watson Knowledge Studio"},{"content":"Co-Authored by Christina Niegel and Jan Forster\nOne of the IBM Watson services that allow a very deep understanding of complex natural language is Watson Knowledge Studio (WKS). It helps users to design and develop Machine Learning annotators trained on understanding a domain language, not only regarding specific terminology but also underlying related information. Accessing not only strings and keywords, but context-sensitive information can unlock the semantic meaning of terms and phrases and helps to concentrate on what\u0026rsquo;s relevant for the goal of the NLP application.\n# A Type System? A Type System is the abstract representation of information that is relevant within the domain and will be the guideline for the \u0026ldquo;human annotators\u0026rdquo;. They are going to train the system and help WKS to learn what to do with text.\nFor all those who already developed annotators with WKS, it\u0026rsquo;s nothing new when we say: a well-designed Type System can decide about the success or fail of your WKS project. Creating a Type System is often a challenging task and underestimating it can turn out in non-scalable solutions and unsatisfied users of the end product.\nUnfortunately, contrary to common beliefs, we have to admit that there is no recipe for a universal Type System ‚Äî sorry. The number of different Type Systems varies as much as there are business problems and domain languages. But even if there\u0026rsquo;s not a single solution that solves every problem, we\u0026rsquo;d like to share our approach that can ease the process of building a feasible Type System.\n# Use Case Example Imagine yourself being in a situation where your client, a large smartphone retailer, wants to allow his customers to find smartphones with their natural language using a chatbot. People might ask the following questions to find a suitable smartphone:\n\u0026ldquo;Which iPhone can I get for less than 500 dollars?\u0026rdquo; \u0026ldquo;Which phone has more than 4GB of RAM?\u0026rdquo; \u0026ldquo;How many pixels does the Pixel have?\u0026rdquo;\nYour client needs to extract the relevant information from the customers\u0026rsquo; questions to provide only smartphone suggestions that meet their criteria.\n# Goals of this Blog Entry Given this use case, we want to share our experiences of the domain adaptation process:\nShare our method to build a robust Type System that is focused on the business requirements and outcome. Questions to ask yourself: What are the entities and relations I need to recognize and extract to achieve the business goal? Support you in abstracting away from a specific example towards a scalable Type System design process. Underline the importance of the mantra: \u0026ldquo;Consistency is key\u0026rdquo;.\n# WKS Terminology Before we get started, let\u0026rsquo;s make sure we are using the same terms in the form of a \u0026ldquo;mini glossary\u0026rdquo;. More info can be found in the official WKS documentation.\n# Type System The Type System defines elements you want to label using annotations. It defines how content can be annotated using entities and relations.\n# Mention A mention is a span of text that you consider relevant in your text.\n# Entity Type An Entity Type is a category you want to be able to recognize in your text to solve your business problem. A mention in the text that belongs to a particular Entity Type is called Entity Mention.\n# Entity Subtype Entity Subtypes allow the further refinement of Entity Types.\n# Relation Type A Relation Type defines a directed relationship between two entities within a single sentence.\n# What is MURAL? No, we\u0026rsquo;re not traveling back in time to the Stone Age. MURAL is an online collaboration whiteboard platform. When we met in client engagement in 2018 after working on various NLP projects involving Watson Knowledge Studio and Watson Discovery Service, we came up with the idea to try out MURAL for our domain adaptation process. We both liked using it for Design Thinking sessions before, so we gave it a try.\n# The Flow # Corpus Studies Before you define entities or relations, you should always read through the domain corpus and other existing language artifacts while having the business problem in mind. This takes time and is a tedious task, but it is necessary. Free your mind from any \u0026ldquo;common\u0026rdquo; Entity Types that you might have already in mind just because you know that domain. This can be misleading and brings a bias to your Type System right from the beginning.\n# Mention Collection Post the relevant mentions that occur during your \u0026ldquo;corpus studies\u0026rdquo; to your MURAL board.\nIn our example, this could be terms and phrases collected from end-users that want to find a new smartphone.\n# Entity Type Definition When you have covered a representative set of mentions on your board, start to cluster and align the mentions: try to give those clusters names. These would be the Entity Type names.\nReferring to our initial example user question \u0026ldquo;Which iPhone can I get for less than 500 dollars?\u0026rdquo; you can see the relevant Entity Type clusters in our Type System.\n# Relation Type Definition Sketch possible relations between the entity clusters. Here it is important to keep the business goal and the value of the product in mind. The more complex your Type System gets, the more difficult it will be to train and maintain. You don\u0026rsquo;t need to build a domain ontology. Focus on relations that help to solve your business problem.\nIn the image below you can see the possible relations between the relevant entities we defined for our use case.\n# Transfer to WKS When you are ready, transfer the Entity Types and Relation Types into WKS and start annotating some documents. It\u0026rsquo;s completely OK if you run into problems regarding ambiguous mentions, unclear lengths of spans (multi-token Entity Mentions) or missing relations in the first iteration. Usually, you refine your Type System and go through the above steps again.\n# Cheat Sheet Critical mentions, that might be confusing for the team of human annotators, can be collected on a separate MURAL mention board. This board serves as a cheat sheet. It\u0026rsquo;s also a great way to collaborate and document changes and extensions to your Type System. It will save time when syncing, it is easier to lookup than checking dictionaries in WKS or Excel and ensures consistency within your team and your annotations. Avoid duplicates or overlapping Entity Mentions across Entity Type clusters.\nPro Tip üòâ You can use icons in MURAL to highlight difficult or ambiguous examples that human annotators need to know about.\n# That\u0026rsquo;s it We hope you enjoyed this kind of hands-on session creating a WKS Type System and hope it was fun to follow the use case.\nIf you want to build your own Type System in WKS, get started on the IBM Cloud for free today!\n# About Us Christina Niegel and Jan Forster are NLP practitioners in the IBM Watson team in Europe. We help our clients to use the full potential of our Watson products and solutions.\nWe hope this content was helpful to you! Please provide feedback in the comments below. If you have any questions, let us know, we\u0026rsquo;re happy to help.\n","date":"2019-12-02T00:00:00Z","image":"https://iamjanforster.de/p/wks-type-systems-mural/relation_hu16006b2958fd04647afad5286511d56f_90434_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://iamjanforster.de/p/wks-type-systems-mural/","title":"NLP in Practice"},{"content":"In my last post, I wrote about ¬µniverse, a great framework for training game agents using Reinforcement Learning. After training a lot of agents and playing around with ¬µniverse, I found out that training my agent on the game StackTower is not working as good as for other games like e.g. Doodle Jump.\nThe goal of StackTower is to stack as many blocks as possible on the platform. Each block fades in from east ‚Üí west or north ‚Üí south. Once the block reaches a good position, one can drop it by pressing a key. On the left you see me playing the game. I had luck with my first block, but the consecutive blocks were badly positioned.\nBefore we dive into the issue, the agent is facing in this game, let\u0026rsquo;s have a quick recap of Reinforcement Learning.\nReinforcement Learning consists of five basic concepts:\nThe Agent, our actor who is looking to maximize its reward for actions in a given environment. An Environment, which serves as a place for our actor to act in and discover, e.g. a game like Super Mario. A State, that describes the current status of the environment. Actions, which can be performed by the agent to change the state in an environment, like jumping to get a coin in Super Mario. The Reward, that could be earned by performing a certain action in a given state. Applied to StackTower we can see that the reward for this game is quite tricky. As humans, we intuitively know that the (long-term) reward is somehow related to our timing. The better the timing, the bigger the part of the block that remains on the platform. But this information is reflected nowhere in the game score. Whether the whole block is placed on the platform or just a part of it, the score will always be increased by one. Our agent might be able to learn this relationship, but it will take a lot of game episodes to do so.\nI\u0026rsquo;ve analyzed the reward as a function of episodes for StackTower as it can be seen in the following plot. For the episodes that were played by the agent without any bootstrapping (the ones left to the black line), we can see that the reward curve stays pretty flat. Especially when assuming this growth rate for the following episodes, we could expect an average reward of 6 per episode after playing 2000 episodes (orange line). Which means that the agent is able to stack up 6 blocks on top of each other after learning from 2000 episodes.\nOn my machine this would take roughly 48 hours, so I wanted to speed up the whole process by supporting the agent. Therefore I\u0026rsquo;ve been using @unixpickle\u0026rsquo;s Demoverse to record my gameplay. The best score I was able to achieve on my machine, was 12 stacked blocks (red line in the plot). The average stack size was around 9. So I took 10 game recordings and used them to train the agent\u0026rsquo;s policy on those. As we can see in the above chart, the agent started to rapidly increase its average reward after updating the policy (the curve right to the black line).\nAnd this is the agent playing StackTower after training on 2000 episodes, using bootstrapping after episode 800.\nIf you enjoyed this post, please let me know. Follow me on Medium for the latest updates or just to say hi.\n","date":"2017-11-13T00:00:00Z","image":"https://iamjanforster.de/p/reinforcement-learning-policies/cover_hu53debeda709fed51297b31d53b392500_176392_120x120_fill_q75_box_smart1.jpg","permalink":"https://iamjanforster.de/p/reinforcement-learning-policies/","title":"Reinforcement Learning"},{"content":"After working a couple of weeks with OpenAI\u0026rsquo;s Gym and Universe I\u0026rsquo;m still very excited to discover and learn all possibilities to train RL agents using those frameworks.\nUnfortunately, it seems that there isn\u0026rsquo;t a huge community actively using Universe. I don\u0026rsquo;t know whether that\u0026rsquo;s related to the specific topic of RL or the challenges that come up when working with Universe due to its specific architecture as mentioned e.g. by Alex Nichol:\n‚Ä¶ the biggest problem with Universe is that VNC and Flash need to run in real time. This means that any hiccups on your training machine [‚Ä¶] might suddenly change the frame rate at which your AI experiences its virtual environment.\nTherefore I was looking out for solutions to performance issues I faced when running more complex environments in Universe and came across ¬µniverse. ¬µniverse is developed by Alex Nichol aka unixpickle and providing environments to train RL agents for HTML5 games, which should improve the overall performance and complexity as RL agent training \u0026amp; development framework.\n# Installation The installation of ¬µniverse is pretty straightforward and documented on the ¬µniverse Github page. Unixpickle also developed a ¬µniverse-agent which provides ready-to-use agents based on popular concepts like PPO, TRPO, A3C. Available parameters can be shown by typing the following commands:\nAlgorithm-specific parameters can be found by typing:\n# Training an Agent I\u0026rsquo;ve chosen Doodle Jump as the first game to train A3C agents on, but you can choose from all games provided by ¬µniverse. They can be found in the ¬µniverse games folder. The naming is somehow similar to the naming conventions used in OpenAI\u0026rsquo;s Gym. To start the training process, the following command can be used:\nmuniverse-agent a3c -env DoodleJump-v0 -out doodlejump \u0026raquo; log_doodle.txt 2\u0026gt;\u0026amp;1 The first and second parameters are pretty self-explanatory.\nThe out parameter provides a name where the trained policy is stored to and can be loaded from to continue training.\nAt the end of the command \u0026raquo; log_doodle.text 2\u0026gt;\u0026amp;1 is forwarding the output of the ¬µniverse-agent to a log file we\u0026rsquo;ll use for performance analysis in a next article.\n# Observing the Agent While the training is running you might have taken a look into the muniverse-agent\u0026rsquo;s folder to search for some kind of visualization of the agent\u0026rsquo;s current state. One way to get an understanding of the agent\u0026rsquo;s performance is to take a look at the log file created in the previous step. Another way is to visualize the agent\u0026rsquo;s interactions with the environment. In our case the Doodle Jump gameplay.\nThis can simply be done by stopping the training and adding the flag -record \u0026lt;PATH_TO_RECORDING_FOLDER\u0026gt;. Restarting the agent will create a couple hundred images containing \u0026ldquo;screenshots\u0026rdquo; of the agent\u0026rsquo;s game play.\n# Next Steps In the next article I will show you an easy way to visualize the agent\u0026rsquo;s performance based on the logs we\u0026rsquo;ve created in this little tutorial. I\u0026rsquo;m really looking forward to see what kind of frameworks unixpickle, OpenAI and other companies will work on in the future.\nIf you enjoyed this post, please let me know. Follow me on Medium for the latest updates or just to say hi.\n","date":"2017-11-05T00:00:00Z","image":"https://iamjanforster.de/p/reinforcement-learning-universe/cover_hubce42636ecacc1a380b462f3110efcec_37455_120x120_fill_q75_box_smart1.jpg","permalink":"https://iamjanforster.de/p/reinforcement-learning-universe/","title":"A quick look into ¬µniverse"},{"content":"When I first heard about OpenAI Universe, I wanted to start playing around with it as quickly as possible. Universe was released as\n‚Ä¶, a software platform for measuring and training an AI\u0026rsquo;s general intelligence across the world\u0026rsquo;s supply of games, websites and other applications.\nBut soon I had to realize that Windows isn\u0026rsquo;t probably the best OS to use as a starting point as it is currently not supported by OpenAI Universe. So my first naive idea was to setup up an Ubuntu VM and get the Universe Starter Agent going. Although my machine had a new i7 and 32 GB of memory the results were disappointing ‚Äî even if I only ran 4 agents.\nI\u0026rsquo;ve never managed to reduce the reaction time to less than 60 ms in the Pong environment.\nSo a guest Ubuntu wasn\u0026rsquo;t the way to go. Buying a new SSD and installing Ubuntu would have taken too much time and AWS EC2 wasn\u0026rsquo;t an alternative, so I decided to try out Docker.\n# Setup So I followed the instructions on the OpenAI Universe Github page: To get started, open a Docker Quickstart Terminal as Admin and clone the universe repo:\n1 2 git clone https://github.com/openai/universe.git cd universe Build a docker image, tag it as \u0026lsquo;universe\u0026rsquo;:\n1 docker build -t universe . If the error \u0026ldquo;Error response from daemon: client is newer than server (client API version: 1.24, server API version: 1.23)\u0026rdquo;. pops up, you may have to run the following command to circumvent an issue due to an older Docker Toolbox Installation on Windows 7:\n1 export DOCKER_API_VERSION=1.23 The next step is to start a container from universe image. The -p switches are used to redirect the ports of Tensorboard and the VNC outputs to access them from your Windows machine later on.\n1 docker run --privileged --rm -it -p 12345:12345 -p 5900:5900 -e DOCKER_NET_HOST=172.17.0.1 universe /bin/bash Because of the -it switch and the /bin/bash command you should find yourself in a pseudo tty pointing to the directory /usr/locasl/universe\nA prerequisite for installing the universe-starter-agent is Miniconda.\nDownload the latest installer using wget in your container:\n1 wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh Start the installation typing:\n1 bash ./Miniconda3-latest-Linux-x86_64.sh and confirm the prompts.\nOnce Miniconda is installed, follow the installation instructions given on [https://github.com/openai/universe-starter-agent]\n1 2 3 4 5 6 7 8 9 10 conda create --name universe-starter-agent python=3.5 source /root/minconda3/bin/activate universe-starter-agent apt-get install -y tmux htop cmake golang libjpeg-dev pip install ‚Äúgym[atari]‚Äù pip install universe pip install six pip install tensorflow conda install -y -c https://conda.binstar.org/menpo opencv3 conda install -y numpy conda install -y scipy Now you have everything installed to start using the universe-starter-agent. It\u0026rsquo;s a good time to commit the changes to your universe docker image.\nRun docker ps to get the container id, then run\n1 docker commit \u0026lt;CONTAINER_ID\u0026gt; universe # Test Run Make sure you\u0026rsquo;re in the universe-starter-agent folder and in the correct conda environment by running the following command in your container bash:\n1 cd universe-starter-agent \u0026amp;\u0026amp; source /root/miniconda3/bin/activate universe-starter-agent Start Pacman to see if everything is working correctly by running:\n1 python train.py ‚Äî num-workers 2 ‚Äî env-id gym-core.MsPacman-v0 ‚Äî log-dir /tmp/pacman Now you should see a similar output on your screen: You can now issue the command tmux a to browse through the different sessions that were started to make sure the workers are running correctly. When starting the workers for the first time, they\u0026rsquo;ll pull a container with all the necessary tools needed, which might take a couple of minutes.\nIn tmux you can change between the different processes (workers, tensorboard, orchestrator and htop) by entering CTRL+b [0‚Äì4]. Return to bash by pressing CTRL+b d.\n# Watching your Agents Play If the workers are running correctly and your agents start training, you can view the details in Tensorboard from your Windows machine\u0026rsquo;s browser. The only thing you need is the IP your Docker is running with. You can see the default machine\u0026rsquo;s IP when starting a new Docker Quickstart Terminal:\nJust enter \u0026lt;YOUR_DOCKER_IP\u0026gt;:12345 and Tensorboard should open up.\nTo view the agents playing, use the VNC viewer of your choice and connect it to one of the VNC ports. You can get ports by running docker ps in your Docker Terminal. This returns a list of your primary container and the child containers with your workers. Each worker routes its VNC port 5900 to a free port e.g. 5901 as shown in the figure below.\nWhen connecting to a VNC output, enter openai as password and you should see the agent playing.\nEntering tmux kill-session will stop your workers. If you want to reuse your models in a next training session, think of mounting your Windows filesystem to docker to transfer the log-dir contents to your local machine.\nI hope my little how-to supported you to get OpenAI universe running on your Windows machine.\nIf you enjoyed this post, please let me know. Follow me on Medium for the latest updates or just to say hi.\nImage source: https://www.flickr.com/photos/textfiles/27228418683/ licensed under the Creative Commons Attribution 2.0 Generic\n","date":"2017-10-15T00:00:00Z","image":"https://iamjanforster.de/p/openai-universe/cover_hu52a4e545a3ff0404b6657cacee7f8fed_126968_120x120_fill_q75_box_smart1.jpg","permalink":"https://iamjanforster.de/p/openai-universe/","title":"How to setup OpenAI Universe in Windows using Docker"}]