<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="After working a couple of weeks with OpenAI&rsquo;s Gym and Universe I&rsquo;m still very excited to discover and learn all possibilities to train RL agents using those frameworks. Unfortunately, it seems that there isn&rsquo;t a huge community actively using Universe. I don&rsquo;t know whether that&rsquo;s related to the specific topic of RL or the challenges that come up when working with Universe due to its specific architecture as mentioned e.g. by Alex Nichol:"><title>A quick look into µniverse</title>
<link rel=canonical href=https://iamjanforster.de/p/reinforcement-learning-universe/><link rel=stylesheet href=/scss/style.min.a319ded9d7b084bc9062b1aa25e64217746e83d3362495ec2320885037a3fe48.css><meta property='og:title' content="A quick look into µniverse"><meta property='og:description' content="After working a couple of weeks with OpenAI&rsquo;s Gym and Universe I&rsquo;m still very excited to discover and learn all possibilities to train RL agents using those frameworks. Unfortunately, it seems that there isn&rsquo;t a huge community actively using Universe. I don&rsquo;t know whether that&rsquo;s related to the specific topic of RL or the challenges that come up when working with Universe due to its specific architecture as mentioned e.g. by Alex Nichol:"><meta property='og:url' content='https://iamjanforster.de/p/reinforcement-learning-universe/'><meta property='og:site_name' content="Jan's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='OpenAI Universe'><meta property='article:tag' content='Reinforcement Learning'><meta property='article:tag' content='AI'><meta property='article:published_time' content='2017-11-05T00:00:00+00:00'><meta property='article:modified_time' content='2017-11-05T00:00:00+00:00'><meta property='og:image' content='https://iamjanforster.de/p/reinforcement-learning-universe/cover.jpg'><meta name=twitter:title content="A quick look into µniverse"><meta name=twitter:description content="After working a couple of weeks with OpenAI&rsquo;s Gym and Universe I&rsquo;m still very excited to discover and learn all possibilities to train RL agents using those frameworks. Unfortunately, it seems that there isn&rsquo;t a huge community actively using Universe. I don&rsquo;t know whether that&rsquo;s related to the specific topic of RL or the challenges that come up when working with Universe due to its specific architecture as mentioned e.g. by Alex Nichol:"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://iamjanforster.de/p/reinforcement-learning-universe/cover.jpg'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_huab46e066d3392afe7e9f6c65a7d0ba76_40648_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Jan's Blog</a></h1><h2 class=site-description>I'm an AI Solution Architect at IBM Germany.
Disclaimer: The postings on this site are my own and don't necessarily represent IBM's positions, strategies or opinions.</h2></div></header><ol class=menu-social><li><a href=https://github.com/eightBEC target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://de.linkedin.com/in/1janforster target=_blank title=linkedin rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li><li><a href=https://medium.com/@8B_EC target=_blank title=medium rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-medium" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M8 9h1l3 3 3-3h1"/><path d="M8 15h2"/><path d="M14 15h2"/><path d="M9 9v6"/><path d="M15 9v6"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#installation>Installation</a></li><li><a href=#training-an-agent>Training an Agent</a></li><li><a href=#observing-the-agent>Observing the Agent</a></li><li><a href=#next-steps>Next Steps</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/reinforcement-learning-universe/><img src=/p/reinforcement-learning-universe/cover_hubce42636ecacc1a380b462f3110efcec_37455_800x0_resize_q75_box.jpg srcset="/p/reinforcement-learning-universe/cover_hubce42636ecacc1a380b462f3110efcec_37455_800x0_resize_q75_box.jpg 800w, /p/reinforcement-learning-universe/cover_hubce42636ecacc1a380b462f3110efcec_37455_1600x0_resize_q75_box.jpg 1600w" width=800 height=1200 loading=lazy alt="Featured image of post A quick look into µniverse"></a></div><div class=article-details><header class=article-category><a href=/categories/tutorial/>Tutorial
</a><a href=/categories/ai/>AI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/reinforcement-learning-universe/>A quick look into µniverse</a></h2></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Nov 05, 2017</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>3 minute read</time></div></footer></div></header><section class=article-content><p>After working a couple of weeks with OpenAI&rsquo;s Gym and Universe I&rsquo;m still very excited to discover and learn all possibilities to train RL agents using those frameworks.</p><p>Unfortunately, it seems that there isn&rsquo;t a huge community actively using Universe. I don&rsquo;t know whether that&rsquo;s related to the specific topic of RL or the challenges that come up when working with Universe due to its specific architecture as mentioned e.g. by Alex Nichol:</p><blockquote><p>… the biggest problem with Universe is that VNC and Flash need to run in real time. This means that any hiccups on your training machine […] might suddenly change the frame rate at which your AI experiences its virtual environment.</p></blockquote><p>Therefore I was looking out for solutions to performance issues I faced when running more complex environments in Universe and came across µniverse. µniverse is developed by Alex Nichol aka unixpickle and providing environments to train RL agents for HTML5 games, which should improve the overall performance and complexity as RL agent training & development framework.</p><h2 id=installation><a href=#installation>#</a>
Installation</h2><p>The installation of µniverse is pretty straightforward and documented on the µniverse Github page.
Unixpickle also developed a µniverse-agent which provides ready-to-use agents based on popular concepts like PPO, TRPO, A3C.
Available parameters can be shown by typing the following commands:</p><p><img src=/p/reinforcement-learning-universe/universe-agent.webp width=460 height=118 srcset="/p/reinforcement-learning-universe/universe-agent_hufff94c5ce872b8c4f5d30a78a9553f77_11732_480x0_resize_q75_h2_box_2.webp 480w, /p/reinforcement-learning-universe/universe-agent_hufff94c5ce872b8c4f5d30a78a9553f77_11732_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy alt="Choosing different agents for µniverse" class=gallery-image data-flex-grow=389 data-flex-basis=935px></p><p>Algorithm-specific parameters can be found by typing:</p><p><img src=/p/reinforcement-learning-universe/universe-agent-models.webp width=540 height=436 srcset="/p/reinforcement-learning-universe/universe-agent-models_hu83dde9516e0255b2985944b93f863551_34654_480x0_resize_q75_h2_box_2.webp 480w, /p/reinforcement-learning-universe/universe-agent-models_hu83dde9516e0255b2985944b93f863551_34654_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy alt="Options for the µniverse A3C agent" class=gallery-image data-flex-grow=123 data-flex-basis=297px></p><h2 id=training-an-agent><a href=#training-an-agent>#</a>
Training an Agent</h2><p>I&rsquo;ve chosen Doodle Jump as the first game to train A3C agents on, but you can choose from all games provided by µniverse. They can be found in the µniverse games folder. The naming is somehow similar to the naming conventions used in OpenAI&rsquo;s Gym.
To start the training process, the following command can be used:</p><p>muniverse-agent a3c -env DoodleJump-v0 -out doodlejump &#187; log_doodle.txt 2>&amp;1
The first and second parameters are pretty self-explanatory.</p><p>The out parameter provides a name where the trained policy is stored to and can be loaded from to continue training.</p><p>At the end of the command &#187; log_doodle.text 2>&amp;1 is forwarding the output of the µniverse-agent to a log file we&rsquo;ll use for performance analysis in a next article.</p><h2 id=observing-the-agent><a href=#observing-the-agent>#</a>
Observing the Agent</h2><p>While the training is running you might have taken a look into the muniverse-agent&rsquo;s folder to search for some kind of visualization of the agent&rsquo;s current state. One way to get an understanding of the agent&rsquo;s performance is to take a look at the log file created in the previous step. Another way is to visualize the agent&rsquo;s interactions with the environment. In our case the Doodle Jump gameplay.</p><p><img src=/p/reinforcement-learning-universe/doodle.gif width=320 height=480 srcset="/p/reinforcement-learning-universe/doodle_hu3e6fd1e3da7a2932c0311594a6aa89b4_1997167_480x0_resize_box_1.gif 480w, /p/reinforcement-learning-universe/doodle_hu3e6fd1e3da7a2932c0311594a6aa89b4_1997167_1024x0_resize_box_1.gif 1024w" loading=lazy alt="Agent playing Doodle Jump" class=gallery-image data-flex-grow=66 data-flex-basis=160px>
This can simply be done by stopping the training and adding the flag -record &lt;PATH_TO_RECORDING_FOLDER>. Restarting the agent will create a couple hundred images containing &ldquo;screenshots&rdquo; of the agent&rsquo;s game play.</p><h2 id=next-steps><a href=#next-steps>#</a>
Next Steps</h2><p>In the next article I will show you an easy way to visualize the agent&rsquo;s performance based on the logs we&rsquo;ve created in this little tutorial. I&rsquo;m really looking forward to see what kind of frameworks unixpickle, OpenAI and other companies will work on in the future.</p><p><img src=/human-gameplay.gif loading=lazy alt="Me playing StackTower - trying hard"></p><hr><p>If you enjoyed this post, please let me know. Follow me on <a class=link href=https://medium.com/@8B_EC target=_blank rel=noopener>Medium</a> for the latest updates or just to say hi.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/openai-universe/>OpenAI Universe</a>
<a href=/tags/reinforcement-learning/>Reinforcement Learning</a>
<a href=/tags/ai/>AI</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Nov 05, 2017 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/reinforcement-learning-policies/><div class=article-image><img src=/p/reinforcement-learning-policies/cover.4a079a8f7a3b9876b0fd7a05708c387b_hu53debeda709fed51297b31d53b392500_176392_250x150_fill_q75_box_smart1.jpg width=250 height=150 loading=lazy alt="Featured image of post Reinforcement Learning" data-key=reinforcement-learning-policies data-hash="md5-Sgeaj3o7mHaw/XoFcIw4ew=="></div><div class=article-details><h2 class=article-title>Reinforcement Learning</h2></div></a></article><article class=has-image><a href=/p/openai-universe/><div class=article-image><img src=/p/openai-universe/cover.59ae1e9a712751addb8a846590eceeb4_hu52a4e545a3ff0404b6657cacee7f8fed_126968_250x150_fill_q75_box_smart1.jpg width=250 height=150 loading=lazy alt="Featured image of post How to setup OpenAI Universe in Windows using Docker" data-key=openai-universe data-hash="md5-Wa4emnEnUa3bioRlkOzutA=="></div><div class=article-details><h2 class=article-title>How to setup OpenAI Universe in Windows using Docker</h2></div></a></article><article class=has-image><a href=/p/wks-type-systems-mural/><div class=article-image><img src=/p/wks-type-systems-mural/relation.778f8dc3e30bda130572d9374f3cc12b_hu16006b2958fd04647afad5286511d56f_90434_250x150_fill_q75_h2_box_smart1_2.webp width=250 height=150 loading=lazy alt="Featured image of post NLP in Practice" data-key=wks-type-systems-mural data-hash="md5-d4+Nw+ML2hMFctk3TzzBKw=="></div><div class=article-details><h2 class=article-title>NLP in Practice</h2></div></a></article><article><a href=/p/fastapi-ml-skeleton/><div class=article-details><h2 class=article-title>A simple FastAPI boilerplate for ML models</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2017 -
2024 Jan's Blog</section><section class=powerby>Disclaimer: The postings on this site are my own and don't necessarily represent IBM's positions, strategies or opinions.<br>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.25.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=/js/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=/js/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=/js/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=/js/photoswipe.min.css crossorigin=anonymous></main></div><script src=/js/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script></body></html>