<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Bootstrapping Policies using Human Game Play Recordings"><title>Reinforcement Learning</title>
<link rel=canonical href=https://iamjanforster.de/p/reinforcement-learning-policies/><link rel=stylesheet href=/scss/style.min.a319ded9d7b084bc9062b1aa25e64217746e83d3362495ec2320885037a3fe48.css><meta property='og:title' content="Reinforcement Learning"><meta property='og:description' content="Bootstrapping Policies using Human Game Play Recordings"><meta property='og:url' content='https://iamjanforster.de/p/reinforcement-learning-policies/'><meta property='og:site_name' content="Jan's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='OpenAI Universe'><meta property='article:tag' content='Reinforcement Learning'><meta property='article:tag' content='AI'><meta property='article:published_time' content='2017-11-13T00:00:00+00:00'><meta property='article:modified_time' content='2017-11-13T00:00:00+00:00'><meta property='og:image' content='https://iamjanforster.de/p/reinforcement-learning-policies/cover.jpg'><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="Bootstrapping Policies using Human Game Play Recordings"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://iamjanforster.de/p/reinforcement-learning-policies/cover.jpg'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_huab46e066d3392afe7e9f6c65a7d0ba76_40648_300x0_resize_box_3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Jan's Blog</a></h1><h2 class=site-description>I'm an AI Solution Architect at IBM Germany.
Disclaimer: The postings on this site are my own and don't necessarily represent IBM's positions, strategies or opinions.</h2></div></header><ol class=menu-social><li><a href=https://github.com/eightBEC target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://de.linkedin.com/in/1janforster target=_blank title=linkedin rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li><li><a href=https://medium.com/@8B_EC target=_blank title=medium rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-medium" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 4m0 2a2 2 0 012-2h12a2 2 0 012 2v12a2 2 0 01-2 2H6a2 2 0 01-2-2z"/><path d="M8 9h1l3 3 3-3h1"/><path d="M8 15h2"/><path d="M14 15h2"/><path d="M9 9v6"/><path d="M15 9v6"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/reinforcement-learning-policies/><img src=/p/reinforcement-learning-policies/cover_hu53debeda709fed51297b31d53b392500_176392_800x0_resize_q75_box.jpg srcset="/p/reinforcement-learning-policies/cover_hu53debeda709fed51297b31d53b392500_176392_800x0_resize_q75_box.jpg 800w, /p/reinforcement-learning-policies/cover_hu53debeda709fed51297b31d53b392500_176392_1600x0_resize_q75_box.jpg 1600w" width=800 height=242 loading=lazy alt="Featured image of post Reinforcement Learning"></a></div><div class=article-details><header class=article-category><a href=/categories/tutorial/>Tutorial
</a><a href=/categories/ai/>AI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/reinforcement-learning-policies/>Reinforcement Learning</a></h2><h3 class=article-subtitle>Bootstrapping Policies using Human Game Play Recordings</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Nov 13, 2017</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>3 minute read</time></div></footer></div></header><section class=article-content><p>In my last post, I wrote about µniverse, a great framework for training game agents using Reinforcement Learning. After training a lot of agents and playing around with µniverse, I found out that training my agent on the game StackTower is not working as good as for other games like e.g. Doodle Jump.</p><p><img src=/p/reinforcement-learning-policies/human-gameplay.gif width=320 height=480 srcset="/p/reinforcement-learning-policies/human-gameplay_hucb0b755f4cfbd1f75f9a7d262cd2543c_818483_480x0_resize_box_1.gif 480w, /p/reinforcement-learning-policies/human-gameplay_hucb0b755f4cfbd1f75f9a7d262cd2543c_818483_1024x0_resize_box_1.gif 1024w" loading=lazy alt="Me playing StackTower - trying hard" class=gallery-image data-flex-grow=66 data-flex-basis=160px></p><p>The goal of StackTower is to stack as many blocks as possible on the platform. Each block fades in from east → west or north → south. Once the block reaches a good position, one can drop it by pressing a key. On the left you see me playing the game. I had luck with my first block, but the consecutive blocks were badly positioned.</p><p>Before we dive into the issue, the agent is facing in this game, let&rsquo;s have a quick recap of Reinforcement Learning.</p><hr><p>Reinforcement Learning consists of five basic concepts:</p><ol><li>The <strong>Agent</strong>, our actor who is looking to maximize its reward for actions in a given environment.</li><li>An <strong>Environment</strong>, which serves as a place for our actor to act in and discover, e.g. a game like Super Mario.</li><li>A <strong>State</strong>, that describes the current status of the environment.</li><li><strong>Actions</strong>, which can be performed by the agent to change the state in an environment, like jumping to get a coin in Super Mario.</li><li>The <strong>Reward</strong>, that could be earned by performing a certain action in a given state.</li></ol><hr><p>Applied to StackTower we can see that the reward for this game is quite tricky. As humans, we intuitively know that the (long-term) reward is somehow related to our timing. The better the timing, the bigger the part of the block that remains on the platform. But this information is reflected nowhere in the game score. Whether the whole block is placed on the platform or just a part of it, the score will always be increased by one. Our agent might be able to learn this relationship, but it will take a lot of game episodes to do so.</p><p><img src=/p/reinforcement-learning-policies/rl-env-action-agent-state-reward.webp width=450 height=435 srcset="/p/reinforcement-learning-policies/rl-env-action-agent-state-reward_hubd2e0f7ea041ca0cedfc562bbe114633_14896_480x0_resize_q75_h2_box_2.webp 480w, /p/reinforcement-learning-policies/rl-env-action-agent-state-reward_hubd2e0f7ea041ca0cedfc562bbe114633_14896_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy alt="Source: Wikimedia.com, License: CC0 1.0" class=gallery-image data-flex-grow=103 data-flex-basis=248px></p><hr><p>I&rsquo;ve analyzed the reward as a function of episodes for StackTower as it can be seen in the following plot. For the episodes that were played by the agent without any bootstrapping (the ones left to the black line), we can see that the reward curve stays pretty flat. Especially when assuming this growth rate for the following episodes, we could expect an average reward of 6 per episode after playing 2000 episodes (orange line). Which means that the agent is able to stack up 6 blocks on top of each other after learning from 2000 episodes.</p><p><img src=/p/reinforcement-learning-policies/rl-bootstrapping-results.webp width=627 height=598 srcset="/p/reinforcement-learning-policies/rl-bootstrapping-results_hu852b73dadc298991b6ecb147f391ec3a_24612_480x0_resize_q75_h2_box_2.webp 480w, /p/reinforcement-learning-policies/rl-bootstrapping-results_hu852b73dadc298991b6ecb147f391ec3a_24612_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy alt="Episode Reward for Agent 1 (with and without bootstrapping)" class=gallery-image data-flex-grow=104 data-flex-basis=251px></p><hr><p>On my machine this would take roughly 48 hours, so I wanted to speed up the whole process by supporting the agent. Therefore I&rsquo;ve been using @unixpickle&rsquo;s Demoverse to record my gameplay. The best score I was able to achieve on my machine, was 12 stacked blocks (red line in the plot). The average stack size was around 9. So I took 10 game recordings and used them to train the agent&rsquo;s policy on those. As we can see in the above chart, the agent started to rapidly increase its average reward after updating the policy (the curve right to the black line).</p><p>And this is the agent playing StackTower after training on 2000 episodes, using bootstrapping after episode 800.</p><p><img src=/p/reinforcement-learning-policies/rl-system-gameplay.gif width=230 height=345 srcset="/p/reinforcement-learning-policies/rl-system-gameplay_hufc86a0d714d6c93296c8f13dac8a4c78_8095268_480x0_resize_box_1.gif 480w, /p/reinforcement-learning-policies/rl-system-gameplay_hufc86a0d714d6c93296c8f13dac8a4c78_8095268_1024x0_resize_box_1.gif 1024w" loading=lazy alt="The agent playing StackTower — way better than I&rsquo;m able to." class=gallery-image data-flex-grow=66 data-flex-basis=160px></p><hr><p>If you enjoyed this post, please let me know. Follow me on <a class=link href=https://medium.com/@8B_EC target=_blank rel=noopener>Medium</a> for the latest updates or just to say hi.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/openai-universe/>OpenAI Universe</a>
<a href=/tags/reinforcement-learning/>Reinforcement Learning</a>
<a href=/tags/ai/>AI</a></section><section class=article-copyright><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Last updated on Nov 13, 2017 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/reinforcement-learning-universe/><div class=article-image><img src=/p/reinforcement-learning-universe/cover.0d2c634c878fee011814f141cb52c19d_hubce42636ecacc1a380b462f3110efcec_37455_250x150_fill_q75_box_smart1.jpg width=250 height=150 loading=lazy alt="Featured image of post A quick look into µniverse" data-key=reinforcement-learning-universe data-hash="md5-DSxjTIeP7gEYFPFBy1LBnQ=="></div><div class=article-details><h2 class=article-title>A quick look into µniverse</h2></div></a></article><article class=has-image><a href=/p/openai-universe/><div class=article-image><img src=/p/openai-universe/cover.59ae1e9a712751addb8a846590eceeb4_hu52a4e545a3ff0404b6657cacee7f8fed_126968_250x150_fill_q75_box_smart1.jpg width=250 height=150 loading=lazy alt="Featured image of post How to setup OpenAI Universe in Windows using Docker" data-key=openai-universe data-hash="md5-Wa4emnEnUa3bioRlkOzutA=="></div><div class=article-details><h2 class=article-title>How to setup OpenAI Universe in Windows using Docker</h2></div></a></article><article class=has-image><a href=/p/wks-type-systems-mural/><div class=article-image><img src=/p/wks-type-systems-mural/relation.778f8dc3e30bda130572d9374f3cc12b_hu16006b2958fd04647afad5286511d56f_90434_250x150_fill_q75_h2_box_smart1_2.webp width=250 height=150 loading=lazy alt="Featured image of post NLP in Practice" data-key=wks-type-systems-mural data-hash="md5-d4+Nw+ML2hMFctk3TzzBKw=="></div><div class=article-details><h2 class=article-title>NLP in Practice</h2></div></a></article><article><a href=/p/fastapi-ml-skeleton/><div class=article-details><h2 class=article-title>A simple FastAPI boilerplate for ML models</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2017 -
2024 Jan's Blog</section><section class=powerby>Disclaimer: The postings on this site are my own and don't necessarily represent IBM's positions, strategies or opinions.<br>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.25.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=/js/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=/js/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=/js/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=/js/photoswipe.min.css crossorigin=anonymous></main></div><script src=/js/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script></body></html>